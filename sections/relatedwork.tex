\section{Related Work}

In this section, we summarize the related work on face spoofing attacks, including adversarial attacks against face authentication systems and spoofing attacks against depth sensors.

\subsection{Adversarial attacks against face recognition systems}
In the earlier time,  adversaries use photos~\cite{chakka2011competition, anjos2011counter}, videos~\cite{raghavendra2015presentation}, and 3D masks~\cite{bhattacharjee2018spoofing, nesli2013spoofing}, a.k.a., facial presentation attacks, to spoof face authentication systems, which however can be well detected and defended by today's deep learning algorithms. 
Deep-learning-based face authentication systems are effective in detecting those facial presentation attacks yet vulnerable to adversarial attacks, and much prior work has demonstrated the feasibility of spoofing face authentication systems with adversarial examples in both digital and physical worlds.

\textbf{Digital adversarial attacks} usually employ subtle adversarial perturbations at the pixel level, and spoof face authentication systems without human perception. Compared to white-box attacks, black-box ones are more challenging. In this area, DFANet~\cite{zhong2020towards} applied adversarial examples to black-box models by using the transferability of the adversarial attacks. Dong et al. \cite{dong2019efficient} proposed an evolutionary optimization method to generate adversarial faces against decision-based black-box models.

\textbf{Physical adversarial attacks} focus on their capabilities to be deployed in the real physical world.  In this area, the adversarial patch draws much attention since pixel-level adversarial perturbations are difficult to achieve in the physical world. %Advhat~\cite{komkov2021advhat} proposed an adversarial patch taped to a hat that can spoof the face authentication system in the physical world. 
A common method is to attach or print the adversarial patch on wearable stuffs such as  eyeglasses  ~\cite{sharif2016accessorize,singh2022powerful}, face masks~\cite{zolfi2021adversarial}, hats~\cite{komkov2021advhat}, stickers~\cite{guo2021meaningful}, etc, to spoofing face authentication systems.
%Guo et al. ~\cite{guo2021meaningful} disguised adversarial patches on the face as stickers, making them more stealthy when attacking.
Another method is to use external light sources to produce adversarial patterns. Nguyen et al.~\cite{nguyen2020adversarial} projected adversarial patterns onto faces to impersonate or obfuscate targets. Vla~\cite{shen2019vla} projected
adversarial perturbations onto the full face and composed a face with the target features.


Compared with prior work that mainly focuses on spoofing the face comparison step, our work tries to fool the face authentication system with a printed photo of a legitimate user by bypassing its liveness detection step.


\subsection{Spoofing attacks against depth sensors} 
Common depth sensors that can acquire information about the depth of a target include structured light depth cameras,  stereo cameras, LiDARs, etc.
In the area of spoofing depth sensors, much work has been done on the LiDAR~\cite{cao2019adversarial, sun2020towards, tu2020physically} by actively emitting laser signals and utilizing the vulnerability of its deep learning algorithms.
DoubleStar~\cite{277102} exploited the weakness of the stereo matching and used it to manipulate the drone, but it can only produce coarse-grained fake depths. 
Our work is the first one to spoof structured light depth cameras and can forge fine-grained depth information. In addition to the face authentication system analyzed in this paper, our work can be extended to other systems equipped with structured-light-based depth cameras.